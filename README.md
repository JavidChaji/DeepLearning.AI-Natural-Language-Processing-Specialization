# Natural Language Processing Specialization



## Natural Language Processing with Classification and Vector Spaces
### Week 1 : Sentiment Analysis with Logistic Regression
#### Lecture: Logistic Regression
- [x] Video : Welcome to the NLP Specialization
- [x] Video : Welcome to course 1
- [x] Reading : Acknowledgment - Ken Church
- [x] Video : Week Introduction
- [x] Video : Supervised ML & Sentiment Analysis
- [x] Reading : Supervised ML & Sentiment Analysis
- [x] Video : Vocabulary & Feature Extraction
- [x] Reading : Vocabulary & Feature Extraction
- [x] Video : Negative and Positive Frequencies
- [x] Video : Feature Extraction with Frequencies
- [x] Reading : Feature Extraction with Frequencies
- [x] Video : Preprocessing
- [x] Reading : Preprocessing
- [ ] Lab : Natural Language preprocessing
- [x] Video : Putting it All Together
- [x] Reading : Putting it all together
- [ ] Lab : Visualizing word frequencies
- [x] Video : Logistic Regression Overview
- [x] Reading : Logistic Regression Overview
- [x] Video : Logistic Regression: Training
- [x] Reading : Logistic Regression: Training
- [ ] Lab : Visualizing tweets and Logistic Regression models
- [x] Video : Logistic Regression: Testing
- [x] Reading : Logistic Regression: Testing
- [x] Video : Logistic Regression: Cost Function
- [x] Reading : Optional Logistic Regression: Cost Function
- [x] Video : Week Conclusion
- [x] Reading : Optional Logistic Regression: Gradient
- [x] Ungraded App Item : Intake Survey)
- [x] Have questions, issues or ideas? Join our Community!
#### Lecture Notes (Optional)
- [x] Reading : Lecture Notes W1
#### Practice Quiz
- [x] Practice Quiz : Logistic Regression
#### Assignment: Sentiment Analysis with Logistic Regression
- [x] Reading : (Optional) Downloading your Notebook, Downloading your Workspace and Refreshing your Workspace
- [ ] Programming Assignment : Logistic Regression
#### Heroes of NLP: Chris Manning (Optional)
- [x] Video : Andrew Ng with Chris Manning
### Week 2 : Sentiment Analysis with Naïve Bayes
#### Lecture: Naive Bayes
- [x] Video : Week Introduction
- [x] Video : Probability and Bayes’ Rule
- [x] Reading : Probability and Bayes’ Rule
- [x] Video : Bayes’ Rule
- [x] Reading : Bayes' Rule
- [x] Video : Naïve Bayes Introduction
- [x] Reading : Naive Bayes Introduction
- [x] Video : Laplacian Smoothing
- [x] Reading : Laplacian Smoothing
- [x] Video : Log Likelihood, Part 1
- [x] Reading : Log Likelihood, Part 1
- [x] Video : Log Likelihood, Part 2
- [x] Reading : Log Likelihood Part 2
- [x] Video : Training Naïve Bayes
- [x] Reading : Training naïve Bayes
- [ ] Lab : Visualizing likelihoods and confidence ellipses
- [x] Video : Testing Naïve Bayes
- [x] Reading : Testing naïve Bayes
- [x] Video : Applications of Naïve Bayes
- [x] Reading : Applications of Naive Bayes
- [x] Video : Naïve Bayes Assumptions
- [x] Reading : Naïve Bayes Assumptions
- [x] Video : Error Analysis
- [x] Reading : Error Analysis
- [x] Video : Week Conclusion
#### Lecture Notes (Optional)
- [x] Lecture Notes W2
#### Practice Quiz
- [x] Naive Bayes
#### Assignment: Naive Bayes
- [ ] Programming Assignment : Naive Bayes
### Week 3 : Vector Space Models
#### Lecture: Vector Space Models
- [x] Video : Week Introduction
- [x] Video : Vector Space Models
- [x] Reading : Vector Space Models
- [x] Video : Word by Word and Word by Doc.
- [x] Reading : Word by Word and Word by Doc.
- [ ] Lab : Linear algebra in Python with Numpy
- [x] Video : Euclidean Distance
- [x] Reading : Euclidian Distance
- [x] Video : Cosine Similarity: Intuition
- [x] Reading : Cosine Similarity: Intuition
- [x] Video : Cosine Similarity
- [x] Reading : Cosine Similarity
- [x] Video : Manipulating Words in Vector Spaces
- [x] Reading : Manipulating Words in Vector Spaces
- [ ] Lab : Manipulating word embeddings
- [x] Video : Visualization and PCA
- [x] Reading : Visualization and PCA
- [x] Video : PCA Algorithm
- [x] Reading : PCA algorithm
- [ ] Lab : Another explanation about PCA
- [x] Reading : The Rotation Matrix (Optional Reading)
- [x] Video : Week Conclusion
#### Lecture Notes (Optional)
- [x] Reading : Lecture Notes W3
#### Practice Quiz
- [x] Practice Quiz : Vector Space Models
#### Assignment: Vector Space Models
- [ ] Programming Assignment : Assignment: Vector Space Models
### Week 4 : Machine Translation and Document Search
#### Lecture: Machine Translation
- [x] Video : Week Introduction
- [x] Video : Overview
- [x] Video : Transforming word vectors
- [x] Reading : Transforming word vectors
- [ ] Lab : Rotation matrices in R2
- [x] Video : K-nearest neighbors
- [x] Reading : K-nearest neighbors
- [x] Video : Hash tables and hash functions
- [x] Reading : Hash tables and hash functions
- [x] Video : Locality sensitive hashing
- [x] Reading : Locality sensitive hashing
- [x] Video : Multiple Planes
- [x] Reading : Multiple Planes
- [ ] Lab : Hash tables
- [x] Video : Approximate nearest neighbors
- [x] Reading : Approximate nearest neighbors
- [x] Video : Searching documents
- [x] Reading : Searching documents
- [x] Video : Week Conclusion
#### Lecture Notes (Optional)
- [x] Reading : Lecture Notes W4
#### Practice Quiz
- [x] Practice Quiz : Hashing and Machine Translation
#### Assignment: Machine Translation
- [ ] Programming Assignment : Word Translation
#### Acknowledgments and Bibliography
- [x] Reading : Acknowledgements
- [x] Reading : Bibliography
#### Heroes of NLP: Kathleen McKeown
- [x] Video : Andrew Ng with Kathleen McKeown



## Natural Language Processing with Probabilistic Models
### Week 1
#### Lecture: Autocorrect and Minimum Edit Distance
- [x] Video : Intro to Course 2
- [x] Video : Week Introduction
- [x] Video : Overview
- [x] Reading : Overview
- [x] Video : Autocorrect
- [x] Reading : Autocorrect
- [x] Video : Building the model
- [x] Reading : Building the model
- [ ] Lab : Lecture notebook: Building the vocabulary
- [x] Video : Building the model II
- [x] Reading : Building the model II
- [ ] Lab : Lecture notebook: Candidates from edits
- [x] Video : Minimum edit distance
- [x] Reading : Minimum edit distance
- [x] Video : Minimum edit distance algorithm
- [x] Reading : Minimum edit distance algorithm
- [x] Video : Minimum edit distance algorithm II
- [x] Reading : Minimum edit distance algorithm II
- [x] Video : Minimum edit distance algorithm III
- [x] Reading : Minimum edit distance III
- [x] Video : Week Conclusion
- [x] Ungraded App Item : [IMPORTANT] Have questions, issues or ideas? Join our Community!
#### Lecture Notes (Optional)
- [x] Reading : Lecture Notes W1
#### Quiz: Auto-correct and Minimum Edit Distance
- [x] Practice Quiz : Auto-correct and Minimum Edit Distance
#### Assignment: Autocorrect
- [x] Reading : (Optional) Downloading your Notebook, Downloading your Workspace and Refreshing your Workspace
- [ ] Programming Assignment : Autocorrect
### Week 2
#### Lecture: Part of Speech Tagging
- [x] Video : Week Introduction
- [x] Video : Part of Speech Tagging
- [x] Reading : Part of Speech Tagging
- [ ] Lab : Lecture Notebook - Working with text files
- [x] Video : Markov Chains
- [x] Reading : Markov Chains
- [ ] Video : Markov Chains and POS Tags
- [ ] Reading : Markov Chains and POS Tags
- [ ] Video : Hidden Markov Models
- [ ] Reading : Hidden Markov Models
- [ ] Video : Calculating Probabilities
- [ ] Reading : Calculating Probabilities
- [ ] Video : Populating the Transition Matrix
- [ ] Reading : Populating the Transition Matrix
- [ ] Video : Populating the Emission Matrix
- [ ] Reading : Populating the Emission Matrix
- [ ] Lab : Lecture Notebook - Working with tags and Numpy
- [ ] Video : The Viterbi Algorithm
- [ ] Reading : The Viterbi Algorithm
- [ ] Video : Viterbi: Initialization
- [ ] Reading : Viterbi: Initialization
- [ ] Video : iterbi: Forward Pass
- [ ] Reading : Viterbi: Forward Pass
- [ ] Video : Viterbi: Backward Pass
- [ ] Reading : Viterbi: Backward Pass
- [ ] Video : Week Conclusion
#### Lecture Notes (Optional)
- [ ] Reading : Lecture Notes W2
#### Practice Quiz
- [ ] Practice Quiz : Part of Speech Tagging
#### Assignment: Part of Speech Tagging
- [ ] Programming Assignment : Part of Speech Tagging
### Week 3
#### Lecture: Autocomplete
- [ ] Video : Week Introduction
- [ ] Video : N-Grams: Overview
- [ ] Reading : N-Grams: Overview
- [ ] Video : N-grams and Probabilities
- [ ] Reading : N-grams and Probabilities
- [ ] Video : Sequence Probabilities
- [ ] Reading : Sequence Probabilities
- [ ] Video : Starting and Ending Sentences
- [ ] Reading : Starting and Ending Sentences
- [ ] Lab : Lecture notebook: Corpus preprocessing for N-grams
- [ ] Video : The N-gram Language Model
- [ ] Reading : The N-gram Language Model
- [ ] Video : Language Model Evaluation
- [ ] Lab : Lecture notebook: Building the language model
- [ ] Reading : Language Model Evaluation
- [ ] Video : Out of Vocabulary Words
- [ ] Reading : Out of Vocabulary Words
- [ ] Video : Smoothing
- [ ] Reading : Smoothing
- [ ] Lab : Lecture notebook: Language model generalization
- [ ] Video : Week Summary
- [ ] Reading : Week Summary
- [ ] Video : Week Conclusion
#### Lecture Notes (Optional)
- [ ] Reading : Lecture Notes W3
#### Practice Quiz
- [ ] Practice Quiz : Autocomplete
#### Assignment: Autocomplete
- [ ] Programming Assignment : Autocomplete
### Week 4
#### Lecture: Word Embeddings
- [ ] Video : Week Introduction
- [ ] Video : Overview
- [ ] Reading : Overview
- [ ] Video : Basic Word Representations
- [ ] Reading : Basic Word Representations
- [ ] Video : Word Embeddings
- [ ] Reading : Word Embeddings
- [ ] Video : How to Create Word Embeddings
- [ ] Reading : How to Create Word Embeddings?
- [ ] Video : Word Embedding Methods
- [ ] Reading : Word Embedding Methods
- [ ] Video : Continuous Bag-of-Words Model
- [ ] Reading : Continuous Bag-of-Words Model
- [ ] Video : Cleaning and Tokenization
- [ ] Reading : Cleaning and Tokenization
- [ ] Video : Sliding Window of Words in Python
- [ ] Reading : Sliding Window of Words in Python
- [ ] Video : Transforming Words into Vectors
- [ ] Reading : Transforming Words into Vectors
- [ ] Lab : Lecture Notebook - Data Preparation
- [ ] Video : Architecture of the CBOW Model
- [ ] Reading : Architecture of the CBOW Model
- [ ] Video : Architecture of the CBOW Model: Dimensions
- [ ] Reading : Architecture of the CBOW Model: Dimensions
- [ ] Video : Architecture of the CBOW Model: Dimensions 2
- [ ] Reading : Architecture of the CBOW Model: Dimensions 2
- [ ] Video : Architecture of the CBOW Model: Activation Functions
- [ ] Reading : Architecture of the CBOW Model: Activation Functions
- [ ] Lab : Lecture Notebook - Intro to CBOW model
- [ ] Video : Training a CBOW Model: Cost Function
- [ ] Reading : Training a CBOW Model: Cost Function
- [ ] Video : Training a CBOW Model: Forward Propagation
- [ ] Reading : Training a CBOW Model: Forward Propagation
- [ ] Video : Training a CBOW Model: Backpropagation and Gradient Descent
- [ ] Reading : Training a CBOW Model: Backpropagation and Gradient Descent
- [ ] Lab : Lecture Notebook - Training the CBOW model
- [ ] Video : Extracting Word Embedding Vectors
- [ ] Reading : Extracting Word Embedding Vectors
- [ ] Lab : Lecture Notebook - Word Embeddings
- [ ] Video : Evaluating Word Embeddings: Intrinsic Evaluation
- [ ] Reading : Evaluating Word Embeddings: Intrinsic Evaluation
- [ ] Video : Evaluating Word Embeddings: Extrinsic Evaluation
- [ ] Reading : Evaluating Word Embeddings: Extrinsic Evaluation
- [ ] Lab : Lecture notebook: Word embeddings step by step
- [ ] Video : Conclusion
- [ ] Reading : Conclusion
- [ ] Video : Week Conclusion
#### Lecture Notes (Optional)
- [ ] Reading : Lecture Notes W4
#### Practice Quiz
- [ ] Practice Quiz : Word Embeddings
#### End of access to Lab Notebooks
- [ ] Reading : [IMPORTANT] Reminder about end of access to Lab Notebooks
#### Assignment: Word Embeddings
- [ ] Programming Assignment : Word Embeddings
#### Acknowledgments
- [ ] Reading : Acknowledgments



## Natural Language Processing with Sequence Models
### Week 1
### Week 2
### Week 3
### Week 4



## Natural Language Processing with Attention Models
### Week 1
### Week 2
### Week 3
### Week 4
